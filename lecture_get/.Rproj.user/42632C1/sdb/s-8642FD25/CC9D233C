{
    "collab_server" : "",
    "contents" : "#PART I Data manipulation\n############################################################################\nlibrary(magrittr)\ndata_row<-read.csv(file=\"./test-2.csv\",header=T,sep=\",\",fileEncoding=\"gbk\",stringsAsFactors = F)\n#View(data_row)\ndim(data_row)\n#transform the Inf to NA\ndata_row[sapply(data_row,is.infinite)] <- NA\ndata <-na.omit(data_row)\n\ndata_1 <- data[which(data[,62] !=\"金融业\"),] #delete the financial industry\ndata_1 <- data_1[,-c(1,62)]    #delete irrelevant information including the coding for sample and Industry name\ndim(data_1)\ndata_1$default = data_1$default %>% as.factor\n# get the x's variable and response y\ny <- data_1[,1]\nx <- data_1[,-1]\nN <- length(y)\n\n# depart the total sample into traning data and test data\nTra_Index <- sample(1:N, floor(0.75*N), replace = F) #3/4 traning sample\nx_tra <- x[Tra_Index,]  %>% unlist %>% matrix(ncol=ncol(x))\ny_tra <- y[Tra_Index] \n\nx_test <- x[-Tra_Index,]  %>% unlist %>% matrix(ncol=ncol(x)) \ny_test <- y[-Tra_Index]\n\nTrain_data <- data.frame(y_tra,x_tra)\nnames(Train_data) <- names(data_1)\n\nTest_data <- data.frame(y_test,x_test)\nnames(Test_data) <- names(data_1)\n\n\n#PART II:  Construct the model \n############################################################################\nlibrary(\"glmnet\")\nlibrary(caret)\nlibrary(magrittr)\nlibrary(\"e1071\")\nlibrary(pROC)\n\n#Choose the best tuning parameter\ncv_fit <- cv.glmnet(x_tra, y_tra, family=\"binomial\")# k-fold cv for glmnet\nplot(cv_fit)\nlambda_min <- cv_fit$lambda.min  #the value of lambda that gives minmum cvm\nlambda_lse <- cv_fit$lambda.1se  #largest value of lambda such that error is within 1 standard error of the minimum.\n\n#fit the model using penalizated GLM\nfit_1 <- glmnet(x_tra, y_tra, family = \"binomial\")\nplot(fit_1, xvar = \"lambda\")\ngrid()\n\n#Choose important variable by regularization\ncoef_1 <- coef.glmnet(fit_1, s = cv_fit$lambda.1se)    # extract coefficients at lambda equals to lambda.lse\n\n#PART III:  Model evaluation \n############################################################################\npred_1 <-predict(fit_1, newx=x_test, s=cv_fit$lambda.1se, type = \"class\") %>% as.numeric\n\n#Confusion Matrix\nconfusionMatrix(pred_1,y_test)\nlibrary(ROSE)\naccuracy.meas(y_test,pred_1)#计算准确率，召回率和F测度等\nroc.curve(y_test,pred_1,plotit = F)# 0.5分类效果很差\n\n#Imbalance data manipulation\n#过采样方法\ndata_balanced_over <- ovun.sample(default~., data = Train_data, method = \"over\", N=2700)$data#为0的样本是2570个，补充到正样本与负样本量一样大\ntable(data_balanced_over$default)\n\nx_over_tra <- data_balanced_over[,-1] %>% as.matrix\ny_over_tra <- data_balanced_over[,1] \n\ncv_fit_glm_over <- cv.glmnet(x_over_tra, y_over_tra, family=\"binomial\")\nplot(cv_fit_glm_over)\nlambda_lse_glm_over <- cv_fit_glm_over$lambda.1se\n\n\nfit_glm_over <- glmnet(x_over_tra, y_over_tra , family = \"binomial\")#训练\ncoef_glm_over <- coef.glmnet(fit_glm_over, s = lambda_lse_glm_over) \npred_glm_over <- predict(fit_glm_over,newx = x_test, s = lambda_lse_glm_over,type = \"class\")\n\naccuracy.meas(y_test,pred_glm_over)\nconfusionMatrix(pred_glm_over,y_test)\n\n#欠采样方法\ndata_balanced_under <- ovun.sample(default~., data = Train_data, method = \"under\", N=100, seed = 1)$data#为1的样本量是34，两倍是68\ntable(data_balanced_under$default)\n\nx_under_tra <- data_balanced_under[,-1] %>% as.matrix\ny_under_tra <- data_balanced_under[,1] \n\ncv_fit_glm_under <- cv.glmnet(x_under_tra, y_under_tra, family=\"binomial\")\nplot(cv_fit_glm_under)\nlambda_lse_glm_under <- cv_fit_glm_under$lambda.1se\n\n\nfit_glm_under <- glmnet(x_under_tra, y_under_tra , family = \"binomial\")#训练\ncoef_glm_under <- coef.glmnet(fit_glm_under, s = lambda_lse_glm_under) \npred_glm_under <- predict(fit_glm_under, newx = x_test, s = lambda_lse_glm_under, type = \"class\")\n\naccuracy.meas(y_test,pred_glm_under)\nconfusionMatrix(pred_glm_under,y_test)\n\n\n\n#同时进行过采样和欠采样\ndata_balanced_both <- ovun.sample(default~., data = Train_data, method = \"both\", p=0.1,N=2604,seed = 1)$data#函数的参数p代表新生成数据集中正类的比例。\ntable(data_balanced_both$default)\n\nx_both_tra <- data_balanced_both[,-1] %>% as.matrix\ny_both_tra <- data_balanced_both[,1] \n\ncv_fit_glm_both <- cv.glmnet(x_both_tra, y_both_tra, family=\"binomial\")\nplot(cv_fit_glm_both)\nlambda_lse_glm_both <- cv_fit_glm_both$lambda.1se\n\n\nfit_glm_both <- glmnet(x_both_tra, y_both_tra , family = \"binomial\")#训练\ncoef_glm_both <- coef.glmnet(fit_glm_both, s = lambda_lse_glm_both) \npred_glm_both <- predict(fit_glm_both, newx = x_test, s = lambda_lse_glm_both, type = \"class\")\n\naccuracy.meas(y_test,pred_glm_both)\nconfusionMatrix(pred_glm_both,y_test)\n\n#人工合成法\ndata_balanced_syn <- ROSE(default~., data = Train_data, seed = 1)$data\ntable(data_balanced_syn$default)\n\nx_syn_tra <- data_balanced_syn[,-1] %>% as.matrix\ny_syn_tra <- data_balanced_syn[,1] \n\ncv_fit_glm_syn <- cv.glmnet(x_syn_tra, y_syn_tra, family=\"binomial\")\nplot(cv_fit_glm_syn)\nlambda_lse_glm_syn <- cv_fit_glm_syn$lambda.1se\n\n\nfit_glm_syn <- glmnet(x_syn_tra, y_syn_tra , family = \"binomial\")#训练\ncoef_glm_syn <- coef.glmnet(fit_glm_syn, s = lambda_lse_glm_syn) \npred_glm_syn <- predict(fit_glm_syn, newx = x_test, s = lambda_lse_glm_syn, type = \"class\")\n\naccuracy.meas(y_test,pred_glm_syn)\nconfusionMatrix(pred_glm_syn,y_test)\n\n############################################################\n#adaboost\n############################################################\n#install.packages(\"adabag\")\nlibrary(adabag)\n#训练样本\nboosting_row <- boosting(default~., data = Train_data)\n#预测结果\nboosting_matrix_pred <- table(Train_data$default, predict(boosting_row, Train_data)$class)\nboosting_matrix_pred <- table(Test_data$default,predict(boosting_row,Test_data)$class)\n#计算误差率\n(E_boosting=(sum(boosting_matrix_pred)-sum(diag(boosting_matrix_pred)))/sum(boosting_matrix_pred))\n#0.001152074\n#画出变量重要性图\nbarplot(boosting_row$importance)\n#计算全体的误差演变\nb <- errorevol(boosting_row, Train_data)\nplot(b$error, type = \"l\",main = \"AdaBoost error vs number of trees\")#对误差演变进行画图\n\n############################################################\n#bagging()\n############################################################\nbagging_row <- bagging(default~., data = Train_data)##建立bagging分类模型\n(bagging_matrix_pred <- table(Train_data$default, predict(bagging_row, Train_data)$class))\n#计算误差率\n(E_bagging=(sum(bagging_matrix_pred)-sum(diag(bagging_matrix_pred)))/sum(bagging_matrix_pred))\n#画出变量重要性图\nbarplot(bagging_row$importance)\n",
    "created" : 1529946430142.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3853371074",
    "id" : "CC9D233C",
    "lastKnownWriteTime" : 1527338878,
    "last_content_update" : 1527338878,
    "path" : "C:/Users/heyouxin/Desktop/信用评级_比赛/处理代码.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}